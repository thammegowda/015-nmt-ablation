model_args: # model construction args
  src_vocab: 32000
  tgt_vocab: 32000
  enc_layers: 1
  dec_layers: 1
  hid_size: 512
  ff_size: 2048
  n_heads: 8
  attn_bias: true
  attn_dropout: 0.1
  dropout: 0.1
  activation: gelu
  tied_emb: three-way
  self_attn_rel_pos: 0
model_type: tfmnmt  # model type. tfmnmt is the transformer NMT model

optimizer:
  name: adam
  args:
    betas:
    - 0.9
    - 0.998
    eps: 1.0e-09
    lr: 0.001
    weight_decay: 0

schedule:
  name: noam
  args:
    warmup: 8000
    constant: 2
    model_dim: 512

criterion:
  name: smooth_kld
  args:
    label_smoothing: 0.1

prep: # data preparation
  codec_lib: nlcodec
  char_coverage: 0.99995
  max_types: 32000
  #max_tgt_types: 16000
  min_co_ev: 50
  pieces: bpe   # choices: bpe, char, word, unigram  from google/sentencepiece
  shared_vocab: true  # true means same vocab for src and tgt, false means different vocabs
  src_len: 512   # longer sentences, decision is made as per 'truncate={true,false}'
  tgt_len: 512
  truncate: true   # what to do with longer sentences: if true truncate at src_len or tgt_len; if false filter away
  train_src: data/deu-eng/train.all.deu.tok
  train_tgt: data/deu-eng/train.all.eng.tok
  valid_src: data/deu-eng/tests/Statmt-newstest_deen-2018-deu-eng.deu.tok
  valid_tgt: data/deu-eng/tests/Statmt-newstest_deen-2018-deu-eng.eng.tok
  valid_tgt_raw: data/deu-eng/tests/Statmt-newstest_deen-2018-deu-eng.eng
  max_part_size: 5_000_000

spark: # this block enables spark backend
  spark.master: local[6]  # TODO: change this; set it to available number
  spark.app.name: RTG NMT on Spark
  spark.driver.memory: 60g  #TODO: change this; set it to available number
  spark.serializer: org.apache.spark.serializer.KryoSerializer
  spark.local.dir: /tmp/spark/
  spark.driver.maxResultSize: 0  # dont worry about result size, all are in one machine
  #key1: value1    # any other spark configs you want to control

tester:
  decoder:
    beam_size: 4
    batch_size: 18000  # this is for 1 beam; effective_batch_size = batch_size / beam_size
    lp_alpha: 0.6     # length penalty
    ensemble: 5
    max_len: 50
  suite:
    dev:
    - data/deu-eng/tests/Statmt-newstest_deen-2018-deu-eng.deu.tok
    - data/deu-eng/tests/Statmt-newstest_deen-2018-deu-eng.eng
    nt19:
    - data/deu-eng/tests/Statmt-newstest_deen-2019-deu-eng.deu.tok
    - data/deu-eng/tests/Statmt-newstest_deen-2019-deu-eng.eng
    nt20:
    - data/deu-eng/tests/Statmt-newstest_deen-2020-deu-eng.deu.tok
    - data/deu-eng/tests/Statmt-newstest_deen-2020-deu-eng.eng

trainer:
  init_args:
    chunk_size: 5  # generation in chunks of time steps to reduce memory consumptio
    grad_accum: 1
    clip_grad_norm: 5.0
    fp16: true
  batch_size: [24000, 2000]
  check_point: 1000  # how often to checkpoint?
  keep_models: 10
  steps: 200000
  keep_in_mem: true
  early_stop:
    enabled: true
    by: bleu
    patience: 10
    min_steps: 64000
    signi_round: 2
updated_at: '2021-12-29T11:54:17.348823'
#seed: 12345  
rtg_version:
  previous: 0.6.0
  last_worked: 0.6.1-dev
